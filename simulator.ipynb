{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval_metrics import evaluate_representativeness,evaluate_simplicity, evaluate_diversity,evaluate_incentive_compatibility,evaluate_resistance_to_malicious_behavior,evaluate_resistance_to_collusion,evaluate_robustness\n",
    "\n",
    "from voting_model import Simulation\n",
    "from voting_model import Voter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voter Behaviours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation_with_voter_strategies():\n",
    "    simulation = Simulation()\n",
    "    simulation.initialize_round(30_000_000)\n",
    "    \n",
    "    # Example of setting up voters with different strategies\n",
    "    simulation.round.add_voters([\n",
    "        Voter(voter_id=i, op_available=1000, laziness=0.5, expertise=0.7, strategy='random') for i in range(50)\n",
    "    ])\n",
    "    simulation.round.add_voters([\n",
    "        Voter(voter_id=i, op_available=1000, laziness=0.5, expertise=0.7, strategy='equal_distribution') for i in range(50, 100)\n",
    "    ])\n",
    "    simulation.round.add_voters([\n",
    "        Voter(voter_id=i, op_available=1000, laziness=0.5, expertise=0.7, strategy='top_k') for i in range(100, 150)\n",
    "    ])\n",
    "    \n",
    "    simulation.randomize_projects(600, coi_factor=0)\n",
    "    \n",
    "    simulation.simulate_voting()\n",
    "    \n",
    "    # Evaluate the results\n",
    "    results = simulation.allocate_votes('median', quorum=17, min_amount=1500)\n",
    "    print(results)\n",
    "\n",
    "run_simulation_with_voter_strategies()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_voters=150\n",
    "num_projects=600\n",
    "max_allocation=30000000\n",
    "willingness_to_spend=1\n",
    "laziness_factor=0.6\n",
    "expertise_factor=0.7\n",
    "quorum=17\n",
    "min_amount=1500\n",
    "\n",
    "\n",
    "simulation = Simulation()\n",
    "\n",
    "simulation.initialize_round(max_allocation)\n",
    "simulation.randomize_voters(num_voters, willingness_to_spend, laziness_factor, expertise_factor)\n",
    "simulation.randomize_projects(num_projects, coi_factor=0)\n",
    "simulation.simulate_voting()    \n",
    "results = simulation.allocate_votes('median', quorum, min_amount)\n",
    "print(results)\n",
    "data = simulation.get_project_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>project_id</th>\n",
       "      <th>owner_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>num_votes</th>\n",
       "      <th>score</th>\n",
       "      <th>token_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>0.986013</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>4.466459</td>\n",
       "      <td>80</td>\n",
       "      <td>340783.212468</td>\n",
       "      <td>302362.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>3.626825</td>\n",
       "      <td>54</td>\n",
       "      <td>93162.175855</td>\n",
       "      <td>82658.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>3.428485</td>\n",
       "      <td>52</td>\n",
       "      <td>50907.358320</td>\n",
       "      <td>45167.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>3.842013</td>\n",
       "      <td>60</td>\n",
       "      <td>57123.763511</td>\n",
       "      <td>50683.42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   project_id owner_id    rating  num_votes          score  token_amount\n",
       "0           0     None  0.986013          0       0.000000          0.00\n",
       "1           1     None  4.466459         80  340783.212468     302362.09\n",
       "2           2     None  3.626825         54   93162.175855      82658.74\n",
       "3           3     None  3.428485         52   50907.358320      45167.88\n",
       "4           4     None  3.842013         60   57123.763511      50683.42"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame(data)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal 1: Malicious behaviour Resistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean - Resistance to Malicious Behavior Score: 29999999.994000003\n",
      "median - Resistance to Malicious Behavior Score: 30000000.005499996\n",
      "quadratic - Resistance to Malicious Behavior Score: 29999999.9956\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from voting_model import Voter\n",
    "\n",
    "def evaluate_resistance_to_malicious_behavior(simulation, method, quorum, min_amount, num_tests=100):\n",
    "    results = []\n",
    "    for _ in range(num_tests):\n",
    "        baseline_allocations = run_baseline_simulation(simulation, method, quorum, min_amount)\n",
    "        malicious_allocations = introduce_malicious_voter(simulation, method, quorum, min_amount)\n",
    "        difference = measure_impact(baseline_allocations, malicious_allocations)\n",
    "        results.append(difference)\n",
    "    \n",
    "    avg_difference = np.mean(results)\n",
    "    return avg_difference\n",
    "\n",
    "# Helper functions\n",
    "def run_baseline_simulation(simulation, method, quorum, min_amount):\n",
    "    simulation.simulate_voting()\n",
    "    baseline_allocations = simulation.round.calculate_allocations(method, quorum, min_amount)\n",
    "    simulation.reset_round()\n",
    "    return baseline_allocations\n",
    "\n",
    "def introduce_malicious_voter(simulation, method, quorum, min_amount):\n",
    "    malicious_voter = Voter(voter_id=-1, op_available=simulation.round.max_funding, laziness=0, expertise=1)\n",
    "    malicious_voter.cast_vote(simulation.round.projects[0], malicious_voter.total_op)\n",
    "    simulation.round.voters.append(malicious_voter)\n",
    "    malicious_allocations = simulation.round.calculate_allocations(method, quorum, min_amount)\n",
    "    simulation.reset_round()\n",
    "    return malicious_allocations\n",
    "\n",
    "def measure_impact(baseline_allocations, malicious_allocations):\n",
    "    difference = np.abs(np.array(baseline_allocations) - np.array(malicious_allocations)).sum()\n",
    "    return difference\n",
    "\n",
    "# Example usage\n",
    "methods = ['mean', 'median', 'quadratic']\n",
    "simulation = Simulation()\n",
    "simulation.initialize_round(30_000_000)\n",
    "simulation.randomize_voters(150, willingness_to_spend=1, laziness_factor=0.6, expertise_factor=0.7)\n",
    "simulation.randomize_projects(600, coi_factor=0)\n",
    "\n",
    "for method in methods:\n",
    "    malicious_behavior_score = evaluate_resistance_to_malicious_behavior(simulation, method, quorum=17, min_amount=1500)\n",
    "    print(f'{method} - Resistance to Malicious Behavior Score: {malicious_behavior_score}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bribery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean - Impact of Bribery Score: 29999999.9923\n",
      "median - Impact of Bribery Score: 30000000.0015\n",
      "quadratic - Impact of Bribery Score: 29999999.9915\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from voting_model import Voter\n",
    "\n",
    "def evaluate_impact_of_bribery(simulation, method, quorum, min_amount, bribe_amount, target_project_id, num_tests=100):\n",
    "    results = []\n",
    "    for _ in range(num_tests):\n",
    "        baseline_allocations = run_baseline_simulation(simulation, method, quorum, min_amount)\n",
    "        bribed_allocations = introduce_bribery(simulation, method, quorum, min_amount, bribe_amount, target_project_id)\n",
    "        difference = measure_impact(baseline_allocations, bribed_allocations)\n",
    "        results.append(difference)\n",
    "    \n",
    "    avg_difference = np.mean(results)\n",
    "    return avg_difference\n",
    "\n",
    "# Helper functions\n",
    "def run_baseline_simulation(simulation, method, quorum, min_amount):\n",
    "    simulation.simulate_voting()\n",
    "    baseline_allocations = simulation.round.calculate_allocations(method, quorum, min_amount)\n",
    "    simulation.reset_round()\n",
    "    return baseline_allocations\n",
    "\n",
    "def introduce_bribery(simulation, method, quorum, min_amount, bribe_amount, target_project_id):\n",
    "    # Select a voter to bribe\n",
    "    bribed_voter = np.random.choice(simulation.round.voters)\n",
    "    \n",
    "    # Ensure the voter has enough balance to cast the bribe amount\n",
    "    if bribed_voter.balance_op >= bribe_amount:\n",
    "        # Reset the voter's votes and balance\n",
    "        bribed_voter.reset_voter()\n",
    "        \n",
    "        # Cast a bribe vote for the target project\n",
    "        target_project = next(p for p in simulation.round.projects if p.project_id == target_project_id)\n",
    "        bribed_voter.cast_vote(target_project, bribe_amount)\n",
    "        \n",
    "        # Redistribute the remaining balance to other projects\n",
    "        remaining_balance = bribed_voter.balance_op\n",
    "        for project in simulation.round.projects:\n",
    "            if project.project_id != target_project_id:\n",
    "                amount = np.random.uniform(0, remaining_balance)\n",
    "                bribed_voter.cast_vote(project, amount)\n",
    "                remaining_balance -= amount\n",
    "    \n",
    "    bribed_allocations = simulation.round.calculate_allocations(method, quorum, min_amount)\n",
    "    simulation.reset_round()\n",
    "    return bribed_allocations\n",
    "\n",
    "def measure_impact(baseline_allocations, bribed_allocations):\n",
    "    difference = np.abs(np.array(baseline_allocations) - np.array(bribed_allocations)).sum()\n",
    "    return difference\n",
    "\n",
    "# Example usage\n",
    "methods = ['mean', 'median', 'quadratic']\n",
    "simulation = Simulation()\n",
    "simulation.initialize_round(30_000_000)\n",
    "simulation.randomize_voters(150, willingness_to_spend=1, laziness_factor=0.6, expertise_factor=0.7)\n",
    "simulation.randomize_projects(600, coi_factor=0)\n",
    "\n",
    "for method in methods:\n",
    "    target_project_id = np.random.choice([project.project_id for project in simulation.round.projects])\n",
    "    bribery_impact_score = evaluate_impact_of_bribery(simulation, method, quorum=17, min_amount=1500, bribe_amount=10000, target_project_id=target_project_id)\n",
    "    print(f'{method} - Impact of Bribery Score: {bribery_impact_score}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean - Impact of Bribery (Voter Perspective): 29999999.997599993\n",
      "mean - Impact of Bribery (Project Perspective): 29999999.997599993\n",
      "median - Impact of Bribery (Voter Perspective): 29999999.9929\n",
      "median - Impact of Bribery (Project Perspective): 29999999.9929\n",
      "quadratic - Impact of Bribery (Voter Perspective): 30000000.0072\n",
      "quadratic - Impact of Bribery (Project Perspective): 30000000.0072\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from voting_model import Voter\n",
    "\n",
    "def evaluate_impact_of_bribery(simulation, method, quorum, min_amount, bribe_amount, target_project_id, num_tests=100):\n",
    "    results_voter_perspective = []\n",
    "    results_project_perspective = []\n",
    "    for _ in range(num_tests):\n",
    "        baseline_allocations = run_baseline_simulation(simulation, method, quorum, min_amount)\n",
    "        \n",
    "        bribed_allocations_voter = introduce_bribery_voter_perspective(simulation, method, quorum, min_amount, bribe_amount, target_project_id)\n",
    "        difference_voter = measure_impact(baseline_allocations, bribed_allocations_voter)\n",
    "        results_voter_perspective.append(difference_voter)\n",
    "        \n",
    "        bribed_allocations_project = introduce_bribery_project_perspective(simulation, method, quorum, min_amount, bribe_amount, target_project_id)\n",
    "        difference_project = measure_impact(baseline_allocations, bribed_allocations_project)\n",
    "        results_project_perspective.append(difference_project)\n",
    "    \n",
    "    avg_difference_voter = np.mean(results_voter_perspective)\n",
    "    avg_difference_project = np.mean(results_project_perspective)\n",
    "    \n",
    "    return avg_difference_voter, avg_difference_project\n",
    "\n",
    "# Helper functions\n",
    "def run_baseline_simulation(simulation, method, quorum, min_amount):\n",
    "    simulation.simulate_voting()\n",
    "    baseline_allocations = simulation.round.calculate_allocations(method, quorum, min_amount)\n",
    "    simulation.reset_round()\n",
    "    return baseline_allocations\n",
    "\n",
    "def introduce_bribery_voter_perspective(simulation, method, quorum, min_amount, bribe_amount, target_project_id):\n",
    "    bribed_voter = np.random.choice(simulation.round.voters)\n",
    "    \n",
    "    if bribed_voter.balance_op >= bribe_amount:\n",
    "        bribed_voter.reset_voter()\n",
    "        target_project = next(p for p in simulation.round.projects if p.project_id == target_project_id)\n",
    "        bribed_voter.cast_vote(target_project, bribe_amount)\n",
    "        \n",
    "        remaining_balance = bribed_voter.balance_op\n",
    "        for project in simulation.round.projects:\n",
    "            if project.project_id != target_project_id:\n",
    "                amount = np.random.uniform(0, remaining_balance)\n",
    "                bribed_voter.cast_vote(project, amount)\n",
    "                remaining_balance -= amount\n",
    "    \n",
    "    bribed_allocations = simulation.round.calculate_allocations(method, quorum, min_amount)\n",
    "    simulation.reset_round()\n",
    "    return bribed_allocations\n",
    "\n",
    "def introduce_bribery_project_perspective(simulation, method, quorum, min_amount, bribe_amount, target_project_id):\n",
    "    project_owner_voter = np.random.choice(simulation.round.voters)\n",
    "    \n",
    "    if project_owner_voter.balance_op >= bribe_amount:\n",
    "        project_owner_voter.reset_voter()\n",
    "        target_project = next(p for p in simulation.round.projects if p.project_id == target_project_id)\n",
    "        project_owner_voter.cast_vote(target_project, bribe_amount)\n",
    "        \n",
    "        remaining_balance = project_owner_voter.balance_op\n",
    "        for project in simulation.round.projects:\n",
    "            if project.project_id != target_project_id:\n",
    "                amount = np.random.uniform(0, remaining_balance)\n",
    "                project_owner_voter.cast_vote(project, amount)\n",
    "                remaining_balance -= amount\n",
    "    \n",
    "    bribed_allocations = simulation.round.calculate_allocations(method, quorum, min_amount)\n",
    "    simulation.reset_round()\n",
    "    return bribed_allocations\n",
    "\n",
    "def measure_impact(baseline_allocations, bribed_allocations):\n",
    "    difference = np.abs(np.array(baseline_allocations) - np.array(bribed_allocations)).sum()\n",
    "    return difference\n",
    "\n",
    "# Example usage\n",
    "methods = ['mean', 'median', 'quadratic']\n",
    "simulation = Simulation()\n",
    "simulation.initialize_round(30_000_000)\n",
    "simulation.randomize_voters(150, willingness_to_spend=1, laziness_factor=0.6, expertise_factor=0.7)\n",
    "simulation.randomize_projects(600, coi_factor=0)\n",
    "\n",
    "for method in methods:\n",
    "    target_project_id = np.random.choice([project.project_id for project in simulation.round.projects])\n",
    "    avg_difference_voter, avg_difference_project = evaluate_impact_of_bribery(\n",
    "        simulation, method, quorum=17, min_amount=1500, bribe_amount=10000, target_project_id=target_project_id\n",
    "    )\n",
    "    print(f'{method} - Impact of Bribery (Voter Perspective): {avg_difference_voter}')\n",
    "    print(f'{method} - Impact of Bribery (Project Perspective): {avg_difference_project}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resistance to collusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluate_impact_of_collusion(simulation, method, quorum, min_amount, collusion_group_size, target_project_ids, num_tests=100):\n",
    "    results = []\n",
    "    for _ in range(num_tests):\n",
    "        baseline_allocations = run_baseline_simulation(simulation, method, quorum, min_amount)\n",
    "        colluded_allocations = introduce_collusion(simulation, method, quorum, min_amount, collusion_group_size, target_project_ids)\n",
    "        difference = measure_impact(baseline_allocations, colluded_allocations)\n",
    "        results.append(difference)\n",
    "    \n",
    "    avg_difference = np.mean(results)\n",
    "    return avg_difference\n",
    "\n",
    "# Helper functions\n",
    "def run_baseline_simulation(simulation, method, quorum, min_amount):\n",
    "    simulation.simulate_voting()\n",
    "    baseline_allocations = simulation.round.calculate_allocations(method, quorum, min_amount)\n",
    "    simulation.reset_round()\n",
    "    return baseline_allocations\n",
    "\n",
    "def introduce_collusion(simulation, method, quorum, min_amount, collusion_group_size, target_project_ids):\n",
    "    colluding_voters = np.random.choice(simulation.round.voters, size=collusion_group_size, replace=False)\n",
    "    \n",
    "    for voter in colluding_voters:\n",
    "        voter.reset_voter()\n",
    "        for project_id in target_project_ids:\n",
    "            target_project = next(p for p in simulation.round.projects if p.project_id == project_id)\n",
    "            amount = np.random.uniform(0, voter.balance_op / len(target_project_ids))\n",
    "            voter.cast_vote(target_project, amount)\n",
    "    \n",
    "    colluded_allocations = simulation.round.calculate_allocations(method, quorum, min_amount)\n",
    "    simulation.reset_round()\n",
    "    return colluded_allocations\n",
    "\n",
    "def measure_impact(baseline_allocations, colluded_allocations):\n",
    "    difference = np.abs(np.array(baseline_allocations) - np.array(colluded_allocations)).sum()\n",
    "    return difference\n",
    "\n",
    "# Example usage\n",
    "methods = ['mean', 'median', 'quadratic']\n",
    "simulation = Simulation()\n",
    "simulation.initialize_round(30_000_000)\n",
    "simulation.randomize_voters(150, willingness_to_spend=1, laziness_factor=0.6, expertise_factor=0.7)\n",
    "simulation.randomize_projects(600, coi_factor=0)\n",
    "\n",
    "for method in methods:\n",
    "    target_project_ids = np.random.choice([project.project_id for project in simulation.round.projects], size=3, replace=False)\n",
    "    collusion_impact_score = evaluate_impact_of_collusion(\n",
    "        simulation, method, quorum=17, min_amount=1500, collusion_group_size=10, target_project_ids=target_project_ids\n",
    "    )\n",
    "    print(f'{method} - Impact of Collusion Score: {collusion_impact_score}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incentive Compatibility: Strategy Proofness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean - Strategyproof Ratio: 0.0\n",
      "median - Strategyproof Ratio: 0.0\n",
      "quadratic - Strategyproof Ratio: 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def evaluate_strategyproofness(simulation, method, quorum, min_amount, strategy, num_tests=100):\n",
    "    results = []\n",
    "    for _ in range(num_tests):\n",
    "        baseline_allocations, baseline_utilities = run_baseline_simulation(simulation, method, quorum, min_amount)\n",
    "        strategic_allocations, strategic_utilities = introduce_strategic_voting(simulation, method, quorum, min_amount, strategy)\n",
    "        is_strategyproof = measure_strategyproofness(baseline_utilities, strategic_utilities)\n",
    "        results.append(is_strategyproof)\n",
    "    \n",
    "    strategyproof_ratio = np.mean(results)\n",
    "    return strategyproof_ratio\n",
    "\n",
    "# Helper functions\n",
    "def calculate_utility(voter, allocations):\n",
    "    utility = 0\n",
    "    for vote in voter.votes:\n",
    "        project_id = vote.project.project_id\n",
    "        project_allocation = allocations[project_id] if project_id < len(allocations) else 0\n",
    "        utility += project_allocation * (vote.amount if vote.amount else 0)\n",
    "    return utility\n",
    "\n",
    "def run_baseline_simulation(simulation, method, quorum, min_amount):\n",
    "    simulation.simulate_voting()\n",
    "    baseline_allocations = simulation.round.calculate_allocations(method, quorum, min_amount)\n",
    "    voter_utilities = {\n",
    "        voter.voter_id: calculate_utility(voter, baseline_allocations)\n",
    "        for voter in simulation.round.voters\n",
    "    }\n",
    "    simulation.reset_round()\n",
    "    return baseline_allocations, voter_utilities\n",
    "\n",
    "def introduce_strategic_voting(simulation, method, quorum, min_amount, strategy):\n",
    "    strategic_voters = simulation.round.voters\n",
    "    \n",
    "    for voter in strategic_voters:\n",
    "        voter.reset_voter()\n",
    "        strategy(voter, simulation.round.projects)\n",
    "    \n",
    "    strategic_allocations = simulation.round.calculate_allocations(method, quorum, min_amount)\n",
    "    voter_utilities = {\n",
    "        voter.voter_id: calculate_utility(voter, strategic_allocations)\n",
    "        for voter in strategic_voters\n",
    "    }\n",
    "    simulation.reset_round()\n",
    "    return strategic_allocations, voter_utilities\n",
    "\n",
    "def measure_strategyproofness(baseline_utilities, strategic_utilities):\n",
    "    strategyproof = True\n",
    "    for voter_id in baseline_utilities:\n",
    "        if strategic_utilities[voter_id] > baseline_utilities[voter_id]:\n",
    "            strategyproof = False\n",
    "            break\n",
    "    return strategyproof\n",
    "\n",
    "# Example strategic voting strategy\n",
    "def strategic_voting_strategy(voter, projects):\n",
    "    # Example strategy: Vote only for the highest rated project\n",
    "    top_project = max(projects, key=lambda p: p.rating)\n",
    "    voter.cast_vote(top_project, voter.total_op)\n",
    "\n",
    "# Example usage\n",
    "methods = ['mean', 'median', 'quadratic']\n",
    "simulation = Simulation()\n",
    "simulation.initialize_round(30_000_000)\n",
    "simulation.randomize_voters(150, willingness_to_spend=1, laziness_factor=0.6, expertise_factor=0.7)\n",
    "simulation.randomize_projects(600, coi_factor=0)\n",
    "\n",
    "for method in methods:\n",
    "    strategyproof_ratio = evaluate_strategyproofness(\n",
    "        simulation, method, quorum=17, min_amount=1500, strategy=strategic_voting_strategy\n",
    "    )\n",
    "    print(f'{method} - Strategyproof Ratio: {strategyproof_ratio}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group Strategy Proofness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean - Group Strategyproof Ratio: 1.0\n",
      "median - Group Strategyproof Ratio: 1.0\n",
      "quadratic - Group Strategyproof Ratio: 1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def evaluate_group_strategyproofness(simulation, method, quorum, min_amount, collusion_group_size, target_project_ids, num_tests=100):\n",
    "    results = []\n",
    "    for _ in range(num_tests):\n",
    "        baseline_allocations, baseline_utilities = run_baseline_simulation(simulation, method, quorum, min_amount)\n",
    "        colluded_allocations, colluded_utilities = introduce_group_collusion(simulation, method, quorum, min_amount, collusion_group_size, target_project_ids)\n",
    "        is_group_strategyproof = measure_group_strategyproofness(baseline_utilities, colluded_utilities)\n",
    "        results.append(is_group_strategyproof)\n",
    "    \n",
    "    group_strategyproof_ratio = np.mean(results)\n",
    "    return group_strategyproof_ratio\n",
    "\n",
    "# Helper functions\n",
    "def calculate_utility(voter, allocations):\n",
    "    utility = 0\n",
    "    for vote in voter.votes:\n",
    "        project_id = vote.project.project_id\n",
    "        project_allocation = allocations[project_id] if project_id < len(allocations) else 0\n",
    "        utility += project_allocation * (vote.amount if vote.amount else 0)\n",
    "    return utility\n",
    "\n",
    "def run_baseline_simulation(simulation, method, quorum, min_amount):\n",
    "    simulation.simulate_voting()\n",
    "    baseline_allocations = simulation.round.calculate_allocations(method, quorum, min_amount)\n",
    "    voter_utilities = {\n",
    "        voter.voter_id: calculate_utility(voter, baseline_allocations)\n",
    "        for voter in simulation.round.voters\n",
    "    }\n",
    "    simulation.reset_round()\n",
    "    return baseline_allocations, voter_utilities\n",
    "\n",
    "def introduce_group_collusion(simulation, method, quorum, min_amount, collusion_group_size, target_project_ids):\n",
    "    colluding_voters = np.random.choice(simulation.round.voters, size=collusion_group_size, replace=False)\n",
    "    \n",
    "    for voter in colluding_voters:\n",
    "        voter.reset_voter()\n",
    "        for project_id in target_project_ids:\n",
    "            target_project = next(p for p in simulation.round.projects if p.project_id == project_id)\n",
    "            amount = np.random.uniform(0, voter.balance_op / len(target_project_ids))\n",
    "            voter.cast_vote(target_project, amount)\n",
    "    \n",
    "    colluded_allocations = simulation.round.calculate_allocations(method, quorum, min_amount)\n",
    "    voter_utilities = {\n",
    "        voter.voter_id: calculate_utility(voter, colluded_allocations)\n",
    "        for voter in colluding_voters\n",
    "    }\n",
    "    simulation.reset_round()\n",
    "    return colluded_allocations, voter_utilities\n",
    "\n",
    "def measure_group_strategyproofness(baseline_utilities, colluded_utilities):\n",
    "    group_strategyproof = True\n",
    "    for voter_id in baseline_utilities:\n",
    "        if voter_id in colluded_utilities and colluded_utilities[voter_id] > baseline_utilities[voter_id]:\n",
    "            group_strategyproof = False\n",
    "            break\n",
    "    return group_strategyproof\n",
    "\n",
    "# Example usage\n",
    "methods = ['mean', 'median', 'quadratic']\n",
    "simulation = Simulation()\n",
    "simulation.initialize_round(30_000_000)\n",
    "simulation.randomize_voters(150, willingness_to_spend=1, laziness_factor=0.6, expertise_factor=0.7)\n",
    "simulation.randomize_projects(600, coi_factor=0)\n",
    "\n",
    "for method in methods:\n",
    "    target_project_ids = np.random.choice([project.project_id for project in simulation.round.projects], size=3, replace=False)\n",
    "    group_strategyproof_ratio = evaluate_group_strategyproofness(\n",
    "        simulation, method, quorum=17, min_amount=1500, collusion_group_size=10, target_project_ids=target_project_ids\n",
    "    )\n",
    "    print(f'{method} - Group Strategyproof Ratio: {group_strategyproof_ratio}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean - Robustness (Average Distance): 2232714.2490736637\n",
      "median - Robustness (Average Distance): 2399706.473329072\n",
      "quadratic - Robustness (Average Distance): 2172412.445962213\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def evaluate_robustness(simulation, method, quorum, min_amount, num_changes=1, num_tests=100):\n",
    "    results = []\n",
    "    for _ in range(num_tests):\n",
    "        baseline_allocations = run_baseline_simulation(simulation, method, quorum, min_amount)\n",
    "        changed_allocations = introduce_random_changes(simulation, method, quorum, min_amount, num_changes)\n",
    "        distance = measure_distance(baseline_allocations, changed_allocations)\n",
    "        results.append(distance)\n",
    "    \n",
    "    avg_distance = np.mean(results)\n",
    "    return avg_distance\n",
    "\n",
    "# Helper functions\n",
    "def run_baseline_simulation(simulation, method, quorum, min_amount):\n",
    "    simulation.simulate_voting()\n",
    "    baseline_allocations = simulation.round.calculate_allocations(method, quorum, min_amount)\n",
    "    simulation.reset_round()\n",
    "    return baseline_allocations\n",
    "\n",
    "def introduce_random_changes(simulation, method, quorum, min_amount, num_changes=1):\n",
    "    changed_voter = np.random.choice(simulation.round.voters)\n",
    "    \n",
    "    for _ in range(num_changes):\n",
    "        project = np.random.choice(simulation.round.projects)\n",
    "        amount = np.random.uniform(0, changed_voter.balance_op)\n",
    "        changed_voter.cast_vote(project, amount)\n",
    "    \n",
    "    changed_allocations = simulation.round.calculate_allocations(method, quorum, min_amount)\n",
    "    simulation.reset_round()\n",
    "    return changed_allocations\n",
    "\n",
    "def measure_distance(baseline_allocations, changed_allocations):\n",
    "    distance = np.linalg.norm(np.array(baseline_allocations) - np.array(changed_allocations))\n",
    "    return distance\n",
    "\n",
    "# Example usage\n",
    "methods = ['mean', 'median', 'quadratic']\n",
    "simulation = Simulation()\n",
    "simulation.initialize_round(30_000_000)\n",
    "simulation.randomize_voters(150, willingness_to_spend=1, laziness_factor=0.6, expertise_factor=0.7)\n",
    "simulation.randomize_projects(600, coi_factor=0)\n",
    "\n",
    "for method in methods:\n",
    "    avg_distance = evaluate_robustness(\n",
    "        simulation, method, quorum=17, min_amount=1500, num_changes=1\n",
    "    )\n",
    "    print(f'{method} - Robustness (Average Distance): {avg_distance}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pareto efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation(simulation, method, quorum, min_amount):\n",
    "    simulation.simulate_voting()\n",
    "    allocations = simulation.round.calculate_allocations(method, quorum, min_amount)\n",
    "    simulation.reset_round()\n",
    "    return allocations\n",
    "\n",
    "def check_pareto_efficiency(voters, allocations, projects):\n",
    "    pareto_violations = 0\n",
    "    for i in range(len(projects)):\n",
    "        for j in range(i + 1, len(projects)):\n",
    "            project_i = projects[i]\n",
    "            project_j = projects[j]\n",
    "            majority_prefers_i = sum(1 for voter in voters if voter.prefers(project_i, project_j)) > len(voters) / 2\n",
    "            majority_prefers_j = sum(1 for voter in voters if voter.prefers(project_j, project_i)) > len(voters) / 2\n",
    "            \n",
    "            if majority_prefers_i and allocations[project_i.project_id] < allocations[project_j.project_id]:\n",
    "                pareto_violations += 1\n",
    "            if majority_prefers_j and allocations[project_j.project_id] < allocations[project_i.project_id]:\n",
    "                pareto_violations += 1\n",
    "    return pareto_violations\n",
    "def evaluate_pareto_efficiency(simulation, method, quorum, min_amount, num_tests=100):\n",
    "    results = []\n",
    "    for _ in range(num_tests):\n",
    "        allocations = run_simulation(simulation, method, quorum, min_amount)\n",
    "        pareto_violations = check_pareto_efficiency(simulation.round.voters, allocations, simulation.round.projects)\n",
    "        results.append(pareto_violations)\n",
    "    \n",
    "    avg_pareto_violations = np.mean(results)\n",
    "    return avg_pareto_violations\n",
    "\n",
    "# Example usage\n",
    "methods = ['mean', 'median', 'quadratic']\n",
    "simulation = Simulation()\n",
    "simulation.initialize_round(30_000_000)\n",
    "simulation.randomize_voters(150, willingness_to_spend=1, laziness_factor=0.6, expertise_factor=0.7)\n",
    "simulation.randomize_projects(600, coi_factor=0)\n",
    "\n",
    "for method in methods:\n",
    "    avg_pareto_violations = evaluate_pareto_efficiency(\n",
    "        simulation, method, quorum=17, min_amount=1500\n",
    "    )\n",
    "    print(f'{method} - Average Pareto Violations: {avg_pareto_violations}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "govxs_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
